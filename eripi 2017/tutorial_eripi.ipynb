{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando uma Rede Neural Convolucional usando o TensorFlow\n",
    "\n",
    "- Arquitetura simples com duas camadas convolucionais\n",
    "- Utilizando um conjunto de imagens de células cervicais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base de imagens de células cervicais\n",
    "#### Células anormais\n",
    "![alt text](abnormal.tif \"Abnormal Cell\")\n",
    "\n",
    "#### Células normais\n",
    "![alt text](normal.tif \"Normal Cell\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construindo a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob \n",
    "import random\n",
    "\n",
    "#Pacotes principais\n",
    "import numpy as np\n",
    "from skimage.io import imread_collection\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' weight_variable(shape)\n",
    "- A entrada dessa função é uma lista no formato [batch,altura,largura,profundidade], na qual batch representa o número\n",
    "de imagens processadas de uma vez. Altura, largura e profundidade representam as dimensões do volume de entrada.\n",
    "- O retorno dessa função são os valores de pesos inicializados de maneira aleatória seguindo uma distribuição normal \n",
    "com desvio padrão 0.1.\n",
    "'''\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' bias_variable(shape)\n",
    "- A entrada dessa função é o número de neurônios de uma camada.\n",
    "- O retorno dessa função são os valores de bias inicializados com 0.1.\n",
    "'''\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' conv2d(x, W)\n",
    "- A entrada dessa função é o volume de entrada (x) e os pesos (W) da camada, ambos no formato \n",
    "[batch,altura,largura,profundidade]. Os pesos da camada são retornados na função weight_variable.\n",
    "- O retorno dessa função é o volume de saída da camada após a operação de convolução.\n",
    "- A variável strides = [1, 1, 1, 1] representa que o passo (stride) da convolução é igual a 1 em cada uma das \n",
    "dimensões.\n",
    "- A variável padding='SAME' representa que a operação de zero padding será utilizada para que o volume de saída tenha \n",
    "a mesma dimensão do volume de entrada.\n",
    "'''\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' max_pool_2x2(x)\n",
    "- A entrada dessa função é o volume de entrada (x) da camada de pooling no formato [batch,altura,largura,profundidade].\n",
    "- O retorno dessa função é o volume de saída da camada após a operação de max-pooling.\n",
    "- A variável ksize = [1, 2, 2, 1] representa que o filtro utilizado na operação de pooling tem tamanho 2x2 na altura \n",
    "e largura, e tamanho 1 na dimensão de batch e profundidade.\n",
    "- A variável strides = [1, 2, 2, 1] representa que o passo (stride) da operação de pooling é igual a 2 na altura e \n",
    "largura, e 1 na dimensão de batch e profundidade.\n",
    "- A variável padding='SAME' representa que a operação de zero padding será utilizada para que o volume de saída tenha \n",
    "dimensão igual a [batch, altura/2, largura/2, profundidade] do volume de entrada.\n",
    "'''\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''A variável x irá armazenar as imagens de entrada da rede. Na lista com parâmetros [None,3,10000], o None é utilizado\n",
    "porque não sabemos a quantidade de imagens de entrada. O 3 representa que as imagens possuem 3 canais. E o 10.000 \n",
    "representa a dimensão das imagens (100x100). '''\n",
    "x = tf.placeholder(tf.float32, [None,3, 10000])\n",
    "\n",
    "'''A variável y_ representa as classes das imagens de entrada. Na lista com parâmetros [None,2], o None é utilizado \n",
    "porque não sabemos a quantidade de imagens de entrada. O 2 representa a quantidade de classes que as imagens estão \n",
    "divididas. '''\n",
    "y_ = tf.placeholder(tf.float32, [None, 2])\n",
    "\n",
    "'''A função tf.reshape redimensiona a variável x para o formato de entrada que o Tensorflow aceita.'''\n",
    "x_image = tf.reshape(x, [-1,100,100,3])\n",
    "\n",
    "'''A variável W_conv1 irá armazenar os pesos da primeira camada convolucional, que terá 32 filtros de tamanho 5x5 e \n",
    "profundidade 3. O volume de entrada dessa camada tem dimensão [batch,100,100,3]. O volume de saída terá dimensão \n",
    "igual a [batch,100,100,32]'''\n",
    "W_conv1 = weight_variable([5, 5, 3, 32])\n",
    "\n",
    "'''A variável b_conv1 irá armazenar os valores de bias para os 32 filtros da primeira camada convolucional.'''\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "'''A função tf.nn.relu aplica a função de ativação Relu no volume de saída da primeira camada convolucional. \n",
    "A variável h_conv1 irá armazenar os valores resultante da primeira camada convolucional.'''\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "'''A variável h_pool1 irá armazenar os valores resultantes após a operação de max-pool. O volume de entrada dessa \n",
    "camada tem dimensão [batch,100,100,32]. O volume de saída terá dimensão igual a [batch,50,50,32]'''\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "'''A variável W_conv2 irá armazenar os pesos da segunda camada convolucional, que terá 64 filtros de tamanho 5x5 e \n",
    "profundidade 32. O volume de entrada dessa camada tem dimensão [batch,50,50,32]. O volume de saída terá dimensão \n",
    "igual a [batch,50,50,64]'''\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "\n",
    "'''A variável b_conv2 irá armazenar os valores de bias para os 64 filtros da segunda camada convolucional.'''\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "'''A função tf.nn.relu aplica a função de ativação Relu no volume de saída da segunda camada convolucional. A variável \n",
    "h_conv2 irá armazenar os valores resultante da segunda camada convolucional.'''\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "'''A variável h_pool2 irá armazenar os valores resultantes após a operação de max-pool. O volume de entrada dessa\n",
    "camada tem dimensão [batch,50,50,64]. O volume de saída terá dimensão igual a [batch,25,25,64]'''\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "'''A variável W_fc1 irá armazenar os pesos da primeira camada totalmente conectada. O volume de entrada dessa camada\n",
    "tem dimensão [batch,25,25,64]. Na lista com parâmetros [40000, 1024], o valor 40.000 é utilizado pois são \n",
    "25*25*64=40.000 conexões. 1024 representa a quantidade de neurônios nessa camada.'''\n",
    "W_fc1 = weight_variable([40000, 1024])\n",
    "\n",
    "'''A variável b_fc1 irá armazenar os valores de bias para os 1024 filtros da primeira camada totalmente conectada.'''\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "'''A função tf.reshape altera o formato do volume de saída da segunda camada de pooling para o formato de entrada da \n",
    "primeira camada totalmente conectada.'''\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 40000])\n",
    "\n",
    "'''A função tf.nn.relu aplica a função de ativação Relu após a multiplicação ponto a ponto entre o volume de entrada \n",
    "e os pesos da primeira camada totalmente conectada.'''\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "'''A variável keep_prob conterá a porcentagem de neurônios que serão ativados na aplicação do dropout durante o \n",
    "treinamento.'''\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "'''A função tf.nn.dropout aplica o dropout no volume resultante após a primeira camada totalmente conectada.'''\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "'''A variável W_fc2 conterá os pesos da segunda camada totalmente conectada. O volume de entrada dessa camada tem \n",
    "1024 valores, referentes a quantidade de neurônios da camada anterior. O segundo parâmetro com valor 2 representa as \n",
    "duas classes que a rede será treinada.'''\n",
    "W_fc2 = weight_variable([1024, 2])\n",
    "\n",
    "'''A variável b_fc2 conterá os valores de bias para os dois neurônios da segunda camada totalmente conectada.'''\n",
    "b_fc2 = bias_variable([2])\n",
    "\n",
    "'''A função tf.matmul realiza a multiplicação ponto a ponto entre o volume de entrada e os pesos da segunda camada \n",
    "totalmente conectada. y_conv é a variável que contém a estrutura da rede.'''\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''A função softmax_cross_entropy_with_logits utiliza a função cross-entropy para calcular o erro entre a saída gerada\n",
    "pela CNN de uma determinada entrada e a sua classe correspondente.'''\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= y_conv, labels = y_))\n",
    "\n",
    "'''A função tf.train.AdamOptimizer atualiza os filtros e pesos da CNN utilizando o backpropagation. A variável \n",
    "train_step será utilizada para realizar o treinamento da rede.'''\n",
    "train_step = tf.train.AdamOptimizer(1e-5).minimize(cross_entropy)\n",
    "\n",
    "'''As duas próximas linhas são utilizadas para computar a predição da CNN e calcular a acurácia obtida.'''\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''A função read_images recebe o endereço da pasta que contém a base de dados e retorna dois vetores, um contendo as \n",
    "imagens e o outro contendo a classe de cada imagem.'''\n",
    "def read_images(path):\n",
    "    classes = glob.glob(path+'*')\n",
    "    im_files = []\n",
    "    size_classes = []\n",
    "    for i in classes:\n",
    "        name_images_per_class = glob.glob(i+'/*')\n",
    "        im_files = im_files+name_images_per_class\n",
    "        size_classes.append(len(name_images_per_class))\n",
    "    labels = np.zeros((len(im_files),len(classes)))\n",
    "    \n",
    "    ant = 0\n",
    "    for id_i,i in enumerate(size_classes):\n",
    "        labels[ant:ant+i,id_i] = 1\n",
    "        ant = i\n",
    "    collection = imread_collection(im_files)\n",
    "    \n",
    "    data = []\n",
    "    for id_i,i in enumerate(collection):\n",
    "        data.append((i.reshape(3,-1)))\n",
    "    return np.asarray(data), np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-c4b336b2599e>:36: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoca 0, acuracia do treinamento = 0.5\n",
      "Epoca 0, acuracia do treinamento = 0.46\n",
      "Epoca 0, acuracia do treinamento = 0.46\n",
      "Epoca 0, acuracia do treinamento = 0.58\n",
      "Epoca 0, acuracia do treinamento = 0.48\n",
      "Epoca 0, acuracia do treinamento = 0.32\n",
      "Epoca 0, acuracia do treinamento = 0.540541\n",
      "Epoca 5, acuracia do treinamento = 0.64\n",
      "Epoca 5, acuracia do treinamento = 0.64\n",
      "Epoca 5, acuracia do treinamento = 0.7\n",
      "Epoca 5, acuracia do treinamento = 0.7\n",
      "Epoca 5, acuracia do treinamento = 0.66\n",
      "Epoca 5, acuracia do treinamento = 0.62\n",
      "Epoca 5, acuracia do treinamento = 0.72973\n",
      "Epoca 10, acuracia do treinamento = 0.7\n",
      "Epoca 10, acuracia do treinamento = 0.74\n",
      "Epoca 10, acuracia do treinamento = 0.8\n",
      "Epoca 10, acuracia do treinamento = 0.74\n",
      "Epoca 10, acuracia do treinamento = 0.68\n",
      "Epoca 10, acuracia do treinamento = 0.7\n",
      "Epoca 10, acuracia do treinamento = 0.837838\n",
      "Epoca 15, acuracia do treinamento = 0.76\n",
      "Epoca 15, acuracia do treinamento = 0.8\n",
      "Epoca 15, acuracia do treinamento = 0.84\n",
      "Epoca 15, acuracia do treinamento = 0.78\n",
      "Epoca 15, acuracia do treinamento = 0.7\n",
      "Epoca 15, acuracia do treinamento = 0.74\n",
      "Epoca 15, acuracia do treinamento = 0.864865\n"
     ]
    }
   ],
   "source": [
    "#A variável path contém o endereço da base de imagens\n",
    "path = '/Users/flavio/Desktop/cells_small/database/'\n",
    "\n",
    "#A variável data irá receber as imagens presente na pasta especificada. Já a variável labels irá receber a classe de cada uma das imagens. \n",
    "data, labels = read_images(path)\n",
    "\n",
    "#A variável batch_size representa o número de imagens que serão processadas a cada passo de treinamento.\n",
    "batch_size = 50\n",
    "\n",
    "#A variável epochs representa o número de épocas de treinamento da rede. Uma época acontece quando todas as imagens do conjunto de treinamento passam pela rede e atualizam seus valores de pesos e filtros.\n",
    "epochs = 16\n",
    "\n",
    "#A variável percent contém a porcentagem de imagens que serão utilizadas para o treinamento.\n",
    "percent = 0.5\n",
    "\n",
    "#Os códigos das próximas 5 linhas estão apenas embaralhando a ordem das imagens e dos labels. \n",
    "data_size = len(data)\n",
    "idx = np.arange(data_size)\n",
    "random.shuffle(idx) \n",
    "data = data[idx]\n",
    "labels = labels[idx]\n",
    "\n",
    "#Formando o conjunto de treinamento com a porcentagem de imagens especificado na variável percent.\n",
    "train = (data[0:np.int(data_size*percent),:,:],labels[0:np.int(data_size*percent),:])\n",
    "\n",
    "#Formando o conjunto de teste com as imagens que não foram utilizadas no treinamento.\n",
    "test = (data[np.int(data_size*(1-percent)):,:,:],labels[np.int(data_size*(1-percent)):,:])\n",
    "\n",
    "#A variável train_size contém o tamanho do conjunto de treinamento.\n",
    "train_size = len(train[0])\n",
    "\n",
    "#Até aqui apenas criamos as variáveis que irão realizar as operações do Tensorflow, porém é necessário criar uma sessão para que elas posam ser executadas.\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#É necessário inicializar todas as variáveis\n",
    "tf.initialize_all_variables().run()\n",
    "\n",
    "#Laço para repetir o processo de treinamento pelo número de épocas especificado.\n",
    "for n in range(epochs):\n",
    "    #Laço para dividir o conjunto de treinamento em sub conjuntos com o tamanho especificado na variável batch_size. Cada iteração desse laço representa um batch.\n",
    "    for i in range(int(np.ceil(train_size/batch_size))):\n",
    "        #As próximas seis linhas de código dividem o conjunto de treinamento nos batchs.\n",
    "        if (i*batch_size+batch_size <= train_size):\n",
    "            batch = (train[0][i*batch_size:i*batch_size+batch_size],\n",
    "                     train[1][i*batch_size:i*batch_size+batch_size])\n",
    "        else:\n",
    "            batch = (train[0][i*batch_size:],\n",
    "                     train[1][i*batch_size:])\n",
    "            \n",
    "        #Chamando a função de treinamento da rede com o valor de dropout igual a 0.5.\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "        \n",
    "        #Exibindo a acurácia obtida utilizando o conjunto de treinamento a cada 5 iterações.\n",
    "        if(n%5 == 0):\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "            print(\"Epoca %d, acuracia do treinamento = %g\"%(n, train_accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação com a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracia =  0.712166\n"
     ]
    }
   ],
   "source": [
    "acuracia = accuracy.eval(feed_dict={x: test[0][:], y_: test[1][:], keep_prob: 1.0})\n",
    "print(\"Acuracia = \", acuracia)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
